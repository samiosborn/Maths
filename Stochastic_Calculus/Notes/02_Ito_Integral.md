# Itô Integral

> **Learning goals**  
> 1) Understand why we need a new definition for stochastic integrals.  
> 2) See how the Itô integral is constructed from Riemann sums.  
> 3) Learn the Itô properties: linearity, martingale, and isometry. 

---

## 1. Motivation

For a deterministic function \(f(t)\), the integral

\[
\int_0^T f(t)\,dt
\]

is defined as the limit of Riemann sums:

\[
\sum_{i=0}^{n-1} f(t_i) \Delta t_i,
\quad \Delta t_i \to 0
\]

Consider a stochastic Wiener process \( W_t \)
- \(W_t\) has independent increments  
- \(W_{t+\Delta t} - W_t \sim \mathcal{N}(0, \Delta t)\)

Between time steps, increments are proportional to \( \sqrt {\Delta t}\), not \(\Delta t\). This is why integrals against \(W_t\) must be defined differently. 

Recall that Brownian motion is nowhere differentiable. If we tried to write: 

\[
dW_t \approx \frac{W_{t+\Delta t}-W_t}{\Delta t} \, dt,
\]

The “derivative” would behave like \(\mathcal{N}(0, 1/\Delta t)\), which blows up as \(\Delta t \to 0\). 

So, the classical Riemann-Stieltjes integral sum does not converge.

---

## 2. Definition

Let \(X_t\) be a process, where: 
- \( X_t \) is a stochastic process
- \( X_t \) is an adapted process (\( X_t \) only depends on \( W_u \) for \( u \leq t \))
- \( X_t \) is square-integrable

Divide \([0,T]\) into a partition \( \Pi \): 
\[
\Pi = \{0=t_0 < t_1 < \dots < t_n = T\}
\]

Define the stochastic Riemann sum: 
\[
S_\Pi = \sum_{k=0}^{n-1} X_{t_k}\,\big(W_{t_{k+1}} - W_{t_k}\big)
\]

If these sums converge in \(L^2\) (mean square) as \(|\Pi|\to 0\), then: 
\[
\int_0^T X_t\,dW_t = \lim_{|\Pi|\to 0} S_\Pi
\]

Important differences from Riemann integral:
- The integrand is evaluated at the left endpoint (\(t_k\)).  
- This ensures the integral is non-anticipative (no knowledge of the future).  
- The limit is in \(L^2\), not almost surely.

---

## 3. Properties

### 3.1 Linearity
\[
\int_0^T (aX_t + bY_t)\,dW_t = a \int_0^T X_t\,dW_t + b \int_0^T Y_t\,dW_t.
\]

#### Reminder: Sigma-Algebra Definition

Let \(\Omega\) be a sample space (the set of all possible outcomes). 

A sigma-algebra \(\mathcal{F}\) on \(\Omega\) is a collection of subsets of \(\Omega\) such that:

1. Contains the whole space:
\[
\Omega \in \mathcal{F}
\]

2. Closed under complementation:
If \(A \in \mathcal{F}\), then the complement \(A^c = \Omega \setminus A \in \mathcal{F}\)

3. Closed under countable unions:  
If \(A_1, A_2, A_3, \dots \in \mathcal{F}\), then:
\[
\bigcup_{i=1}^\infty A_i \in \mathcal{F}
\]

By De Morgan’s laws, this also implies closure under countable intersections.

#### Brownian Motion Sigma Algebra

Let \((W_t)_{t \ge 0}\) be a Wiener process on \((\Omega, \mathcal{F}, \mathbb{P})\).

- For each time \(t \ge 0\), define the sigma-algebra
\[
\mathcal{F}_t = \sigma(W_s : 0 \leq s \leq t)
\]

- This is the filtration generated by Brownian motion, as it contains all events that depend only on the path of \(W\) up to time \(t\).
- Intuitively, \(\mathcal{F}_t\) represents the information available at time \(t\).


### 3.2 Martingale Property
Let \( \mathcal{F}_s \) be the sigma-algebra generated by all Brownian values up to time \( s \).


If \(X_t\) is adapted (depends only on the past), then for \( s < T \)

\[
\mathbb{E}\!\left[\int_0^T X_t\,dW_t \,\middle|\, \mathcal{F}_s\right] 
= \int_0^s X_t\,dW_t
\]

This proves that the Itô integral is a martingale.

Intuition:  
- \(\mathcal{F}_s\) represents the information available at time \(s\). 
- Conditioning on \(\mathcal{F}_s\) means we know the entire past trajectory of \(W_t\) up to \(s\). 

Sketch: If we split the integral's time limits:

\[
\int_0^T X_t\, dW_t 
= \int_0^s X_t\, dW_t \;+\; \int_s^T X_t\, dW_t.
\]

- The first term is already \(\mathcal{F}_s\)-measurable (known at time \(s\)).  
- The second term has expectation 0, conditional on \(\mathcal{F}_s\), because future increments of \(W_t\) are independent of the past.

Therefore:

\[
\mathbb{E}\!\left[\int_0^T X_t\, dW_t \,\middle|\, \mathcal{F}_s\right]
= \int_0^s X_t\,dW_t
\]

#### Reminder: Cauchy Sequence Definition

A sequence \((x_n)\) in a metric space with distance is Cauchy if
\[
\forall \varepsilon > 0, \ \exists N \text{ such that } m,n \geq N \implies d(x_m, x_n) < \varepsilon
\]

- Intuition: terms of the sequence get arbitrarily close to each other. 
- In a complete space (like \(\mathbb{R}\) or \(L^2\)), every Cauchy sequence converges to a limit.

#### Reminder: \(L^2\) Space

For a probability space \((\Omega, \mathcal{F}, \mathbb{P})\),
\[
L^2(\Omega) = \{ X : \Omega \to \mathbb{R} \ \mid\ \mathbb{E}[X^2] < \infty \}.
\]

- Elements of \(L^2\) are random variables with finite variance. 
- Distance: 
\[
d(X, Y) = \mathbb{E}[(X-Y)^2]
\] 
- Norm:
\[
\|X\|_2 = \big(\mathbb{E}[X^2]\big)^{1/2}
\]
- \(L^2\) is a Hilbert space with inner product: 
\[
\langle X, Y \rangle = \mathbb{E}[XY]
\]

#### Reminder: Isometry

An isometry between two normed spaces \((V, \|\cdot\|_V)\) and \((W, \|\cdot\|_W)\) is a map: 
\[
T: V \to W
\]
such that:
\[
\|T(v_1) - T(v_2)\|_W = \|v_1 - v_2\|_V \quad \text{for all } v_1,v_2 \in V
\]

Equivalently, it preserves distances (or norms):
\[
\|T(v)\|_W = \|v\|_V
\]
### 3.3 Itô Isometry

For square-integrable \(X_t\), the Itô map
\[
\mathcal{I} : X \mapsto \int_0^T X_t\, dW_t
\]
is an isometry from
\[
L^2_{\text{adapted}}([0,T]\times\Omega) \quad \to \quad L^2(\Omega)
\]

- On the domain \(L^2_{\text{adapted}}([0,T]\times\Omega)\), the norm is: 
\[
\|X\|^2 = \mathbb{E}\!\left[\int_0^T X_t^2\, dt\right]
\]

- On the codomain \(L^2(\Omega)\), the norm is
  \[
  \|Y\|^2 = \mathbb{E}[Y^2]
  \]

As:
\[
\mathbb{E}\!\left[\Big(\int_0^T X_t\,dW_t\Big)^2\right]
= \mathbb{E}\!\left[\int_0^T X_t^2\,dt\right]
\]


#### Proof: 
Let \(X_t\) be simple, step process on a partition \(0=t_0<\dots< t_n=T \) :

We define:
\[
X_t = \sum_{i=0}^{n-1} \xi_i\,\mathbf{1}_{(t_i, t_{i+1}]}(t)
\]

Where:
- On each interval \((t_i, t_{i+1}]\), the process \(X_t\) takes the constant value: \(\xi_i\) 
- The symbol \(\mathbf{1}_{(t_i,t_{i+1}]}(t)\) is the indicator function, equal to 1 if \(t \in (t_i,t_{i+1}]\), and 0 otherwise.

Using the Itô integral,
\[
\int_0^T X_t\,dW_t
= \sum_{i=0}^{n-1} \xi_i \big(W_{t_{i+1}} - W_{t_i}\big)
= \sum_{i=0}^{n-1} \xi_i \Delta W_i
\]

Square and take expectation:
\[
\mathbb{E}\!\left[\Big(\sum_{i} \xi_i \Delta W_i\Big)^2\right]
= \sum_{i}\mathbb{E}\!\left[\xi_i^2(\Delta W_i)^2\right]
+\sum_{i\neq j}\mathbb{E}\!\left[\xi_i\xi_j\Delta W_i\Delta W_j\right]
\]

Thus, 
- Cross terms vanish: For \(i\neq j\), by independence of disjoint Brownian increments and adaptedness of \(\xi_i,\xi_j\) we have: 
\[
\mathbb{E}\!\left[\xi_i\xi_j\Delta W_i\Delta W_j\right]
= \mathbb{E}\!\left[\xi_i\xi_j\,\mathbb{E}[\Delta W_i\Delta W_j\mid\mathcal F_{\max\{t_i,t_j\}}]\right]
= 0
\]
- Diagonal terms reduce to variances: For each \(i\),
\[
\mathbb{E}\!\left[\xi_i^2(\Delta W_i)^2\right]
= \mathbb{E}\!\left[\xi_i^2\,\mathbb{E}\!\left[(\Delta W_i)^2\mid \mathcal F_{t_i}\right]\right]
= \mathbb{E}\!\left[\xi_i^2\,(t_{i+1}-t_i)\right],
\]
because \(\Delta W_i\sim \mathcal N(0,\,t_{i+1}-t_i)\) is independent of \(\mathcal F_{t_i}\).

Hence for simple processes we have,
\[
\mathbb{E}\!\left[\Big(\int_0^T X_t\,dW_t\Big)^2\right]
= \sum_{i=0}^{n-1}\mathbb{E}\!\left[\xi_i^2\,(t_{i+1}-t_i)\right]
= \mathbb{E}\!\left[\int_0^T X_t^2\,dt\right]
\]

Now to prove for all square-integrable adapted processes: 

Let \(X \in L^2(\Omega\times[0,T])\) be adapted. Then, there exists a sequence of adapted simple processes \(X^{(m)}\) such that: 
\[
\mathbb{E}\!\left[\int_0^T\!\big(X^{(m)}_t - X_t\big)^2\,dt\right]\ \xrightarrow[m\to\infty]{}\ 0
\]

Let:  \(I^{(m)}=\int_0^T X^{(m)}_t\,dW_t\)

As \( X^{(m)}_t \) is a simple process, using the previous half of the proof,
\[
\mathbb{E}\!\left[\big(I^{(m)}-I^{(k)}\big)^2\right]
= \mathbb{E}\!\left[\int_0^T\!\big(X^{(m)}_t - X^{(k)}_t\big)^2\,dt\right]
\]
Now also \(\{I^{(m)}\}\) is Cauchy in \(L^2(\Omega)\) and converges to \(I\).

Since \(X^{(m)} \to X\) in \(L^2\), the right-hand side goes to \(0\). Hence \(\{I^{(m)}\}\) is Cauchy in \(L^2(\Omega)\), and because \(L^2(\Omega)\) is complete, there exists a limit. 

We define:
\[
\int_0^T X_t\, dW_t := I
\]

Since \(I^{(m)} \to I\) in \(L^2\),
\[
\mathbb{E}\!\left[(I^{(m)})^2\right] \xrightarrow[m\to\infty]{} \mathbb{E}\!\left[I^2\right]
\]

On the other hand, because \(X^{(m)} \to X\) in \(L^2\),
\[
\mathbb{E}\!\left[\int_0^T (X^{(m)}_t)^2\,dt\right] \xrightarrow[m\to\infty]{} \mathbb{E}\!\left[\int_0^T X_t^2\,dt\right]
\]

Putting these together,
\[
\mathbb{E}\!\left[\Big(\int_0^T X_t\,dW_t\Big)^2\right]
= \mathbb{E}\!\left[\int_0^T X_t^2\,dt\right]
\]

## 4. Simple Examples

### 4.1 Constant Integrand
If \(X_t \equiv c\),
\[
\int_0^T c\,dW_t = c W_T
\]

### 4.2 Brownian Motion itself
Let \(X_t = W_t\). Then
\[
\int_0^T W_t\,dW_t = \tfrac12\big(W_T^2 - T\big).
\]

Note: This identity comes directly from Itô’s formula (next chapter).

---

## 5. Numerical Approximation

We approximate the Itô integral by discrete sums.

Euler-Riemann approximation: 
\[
\int_0^T X_t\,dW_t \;\approx\; \sum_{k=0}^{n-1} X_{t_k}\,\Delta W_k, 
\quad \Delta W_k \sim \mathcal N(0, \Delta t).
\]
